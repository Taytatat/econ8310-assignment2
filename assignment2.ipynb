{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098b2971-5d7a-4d7f-aec2-3400a31a72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and our model\n",
    "from sklearn.tree import DecisionTreeClassifier #a classifier is different from a gresser, because teh classifer makes decisions for discreet dependent variable\n",
    "#regressor is for continous dependent variables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy as pt \n",
    "\n",
    "from sklearn.metrics import accuracy_score #the proportion of observations for whcih your able to make an accurate decision\n",
    "from sklearn.model_selection import train_test_split #this will help us to break our data into different chunks \n",
    "\n",
    "#what we will use to train the data (data_meal)\n",
    "data_meal = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3.csv\")\n",
    "\n",
    "\n",
    "#this the data we'll make predictions with (for now ignore) data_pred\n",
    "data_pred = pd.read_csv(\" https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff66c588",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m data_meal\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#makes (x) the dependent variable everything but meal which is Y, also drops DateTime and Id because I dont think those are there to help predicy anything but just help find whatever\u001b[39;00m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#remember that capital X, Y are different from lowercase x, y (dont want them to get overwritten)\u001b[39;00m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#x and y are training datat\u001b[39;00m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#xt and yt are testing data\u001b[39;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#will randomly shuffle observations into training and testing bins \u001b[39;00m\n",
      "\u001b[1;32m---> 16\u001b[0m x, xt, y, yt \u001b[38;5;241m=\u001b[39m train_test_split(X, Y,  test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#meal is the DEPENDENT VARIABLE\n",
    "\n",
    "# As usual, Patsy makes data prep easier, use ptsy to make and x and y matrix\n",
    "#<-this is how to do it with matrices: Y, X = pt.dmatrices(\"meal ~ -1 + Iced + Starbucks_DS_Vanilla  + Tea_Half_and_Half + Nuts_Cashews\", data=data_meal) #-1 means no interncept term, meal is the depedent variable (y), everything else are our independent variables (x)\n",
    "\n",
    "\n",
    "#probably have to remove the columns wuth ID and datetime in them? Since they arent applicable\n",
    "y = data_meal['meal'] #this meakes it so that the Depenedt variable (y) meal\n",
    "X = data_meal.drop(['meal', 'id', 'DateTime'], axis =1) #makes (x) the dependent variable everything but meal which is Y, also drops DateTime and Id because I dont think those are there to help predicy anything but just help find whatever\n",
    "\n",
    "\n",
    "#remember that capital X, Y are different from lowercase x, y (dont want them to get overwritten)\n",
    "#x and y are training datat\n",
    "#xt and yt are testing data\n",
    "#will randomly shuffle observations into training and testing bins \n",
    "x, xt, y, yt = train_test_split(X, Y,  test_size=0.33, random_state=42) #train test split is going to reserve 1/3 or .333 of the data as test data so we \n",
    "# can test how well our model performs \n",
    "#randomstate is giving a starting spot for the random draws, allows us to make this repeatable, means that we wil get the same shuffles of data everytime we run this line, becasuse im forcing the random draws to start at the same spot everytime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Total  Discounts  Brewed_Coffee_12_oz  Brewed_Coffee_16_oz  ...  Hot_Chocolate_16_oz  Dr_Pepper  Ocean_Spray_CranGrape  Gum_Peppermint\n",
      "0       3.96        0.0                    0                    0  ...                    0          0                      0               0\n",
      "1       3.57        0.0                    0                    0  ...                    1          0                      0               0\n",
      "2       2.21        0.0                    0                    1  ...                    0          0                      0               0\n",
      "3       2.84        0.0                    0                    0  ...                    0          0                      0               0\n",
      "4       4.58        0.0                    0                    0  ...                    0          0                      0               0\n",
      "...      ...        ...                  ...                  ...  ...                  ...        ...                    ...             ...\n",
      "14164  23.34        0.0                    0                    0  ...                    0          0                      0               0\n",
      "14165   4.63        0.0                    0                    0  ...                    0          0                      0               0\n",
      "14166   4.95        0.0                    0                    0  ...                    0          0                      0               0\n",
      "14167   9.89        0.0                    0                    0  ...                    0          0                      0               0\n",
      "14168   4.63        0.0                    0                    0  ...                    0          0                      0               0\n",
      "\n",
      "[14169 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "#so column names in list\n",
    "column_names = data_meal.columns.tolist()\n",
    "\n",
    "#print(column_names)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c457caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have the training and testing data we can build our model\n",
    "\n",
    "#model is defined by creating an instance of teh decision tree classifier object \n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now fit the model\n",
    "res = model.fit(x,y)\n",
    "\n",
    "modelFit = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In-sample accuracy: 95.85%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now we can make predictions using fitted model (based on your x's it will predict what y's you should get )\n",
    "\n",
    "#THIS IS THE IN-SAMPLE ACCURACY WHICH ISNT REALLY IMPORTANT\n",
    "\n",
    "pred = modelFit.predict(x)\n",
    "print(\"\\n\\nIn-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y, pred), 2))) #the ys are then compared by the accuracy score against truth, and we see how often were right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb26f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In-sample accuracy: 95.85%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Out-of-sample accuracy: 87.77%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#INSTEAD WE WWANT OUT OF SAMPLE ACCURACY\n",
    "\n",
    "#big difference between In and Out of sample. Our model is VERY overfit right now \n",
    "\n",
    "#to fix we are going to try to make the model less complex, model is good sepcifically for our data but not applicable \n",
    "#to all outcomes (so right now our model IS only good for this sepcific instance which is not good)\n",
    "\n",
    "# we will reduce the model to just the generalizable characteritsitcs\n",
    "\n",
    "pred_train = modelFit.predict(x)\n",
    "pred_test = modelFit.predict(xt)\n",
    "\n",
    "#in sample accuracey with training data\n",
    "print(\"\\n\\nIn-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y, pred_train), 2))) #In-sample accuracy: 94.35%\n",
    "\n",
    "#out of sample accuracy with testing data\n",
    "print(\"\\n\\nOut-of-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(yt, pred_test), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25533cba",
   "metadata": {},
   "source": [
    "THIS IS THE FULL CODE ALL STUCK TOGETHER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Total  Discounts  ...  Ocean_Spray_CranGrape  Gum_Peppermint\n",
      "0       3.96        0.0  ...                      0               0\n",
      "1       3.57        0.0  ...                      0               0\n",
      "2       2.21        0.0  ...                      0               0\n",
      "3       2.84        0.0  ...                      0               0\n",
      "4       4.58        0.0  ...                      0               0\n",
      "...      ...        ...  ...                    ...             ...\n",
      "14164  23.34        0.0  ...                      0               0\n",
      "14165   4.63        0.0  ...                      0               0\n",
      "14166   4.95        0.0  ...                      0               0\n",
      "14167   9.89        0.0  ...                      0               0\n",
      "14168   4.63        0.0  ...                      0               0\n",
      "\n",
      "[14169 rows x 81 columns]\n",
      "\n",
      "\n",
      "In-sample accuracy: 95.85%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In-sample accuracy: 95.85%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Out-of-sample accuracy: 87.55%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test (pred) data accuracy: 92.9%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pandas and our model\n",
    "from sklearn.tree import DecisionTreeClassifier #a classifier is different from a gresser, because teh classifer makes decisions for discreet dependent variable\n",
    "#regressor is for continous dependent variables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy as pt \n",
    "\n",
    "from sklearn.metrics import accuracy_score #the proportion of observations for whcih your able to make an accurate decision\n",
    "from sklearn.model_selection import train_test_split #this will help us to break our data into different chunks \n",
    "\n",
    "#what we will use to train the data (data_meal)\n",
    "data_meal = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3.csv\")\n",
    "\n",
    "\n",
    "#this the data we'll make predictions with (for now ignore) data_pred\n",
    "data_pred = pd.read_csv(\" https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#meal is the DEPENDENT VARIABLE\n",
    "\n",
    "# As usual, Patsy makes data prep easier, use ptsy to make and x and y matrix\n",
    "#<-this is how to do it with matrices: Y, X = pt.dmatrices(\"meal ~ -1 + Iced + Starbucks_DS_Vanilla  + Tea_Half_and_Half + Nuts_Cashews\", data=data_meal) #-1 means no interncept term, meal is the depedent variable (y), everything else are our independent variables (x)\n",
    "\n",
    "\n",
    "#probably have to remove the columns wuth ID and datetime in them? Since they arent applicable\n",
    "Y = data_meal['meal'] #this meakes it so that the Depenedt variable (y) meal\n",
    "X = data_meal.drop(['meal', 'id', 'DateTime'], axis =1) #makes (x) the dependent variable everything but meal which is Y, also drops DateTime and Id because I dont think those are there to help predicy anything but just help find whatever\n",
    "\n",
    "\n",
    "#remember that capital X, Y are different from lowercase x, y (dont want them to get overwritten)\n",
    "#x and y are training datat\n",
    "#xt and yt are testing data\n",
    "#will randomly shuffle observations into training and testing bins \n",
    "x, xt, y, yt = train_test_split(X, Y,  test_size=0.33, random_state=42) #train test split is going to reserve 1/3 or .333 of the data as test data so we \n",
    "# can test how well our model performs \n",
    "#randomstate is giving a starting spot for the random draws, allows us to make this repeatable, means that we wil get the same shuffles of data everytime we run this line, becasuse im forcing the random draws to start at the same spot everytime\n",
    "\n",
    "#so column names in list\n",
    "column_names = data_meal.columns.tolist()\n",
    "\n",
    "#print(column_names)\n",
    "\n",
    "print(X)\n",
    "\n",
    "#now that we have the training and testing data we can build our model\n",
    "\n",
    "#model is defined by creating an instance of teh decision tree classifier object \n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "#now fit the model\n",
    "res = model.fit(x,y)\n",
    "\n",
    "modelFit = res\n",
    "\n",
    "\n",
    "#now we can make predictions using fitted model (based on your x's it will predict what y's you should get )\n",
    "\n",
    "#THIS IS THE IN-SAMPLE ACCURACY WHICH ISNT REALLY IMPORTANT\n",
    "\n",
    "pred1 = modelFit.predict(x)\n",
    "print(\"\\n\\nIn-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y, pred1), 2))) #the ys are then compared by the accuracy score against truth, and we see how often were right\n",
    "\n",
    " #INSTEAD WE WWANT OUT OF SAMPLE ACCURACY\n",
    "\n",
    "#big difference between In and Out of sample. Our model is VERY overfit right now \n",
    "\n",
    "#to fix we are going to try to make the model less complex, model is good sepcifically for our data but not applicable \n",
    "#to all outcomes (so right now our model IS only good for this sepcific instance which is not good)\n",
    "\n",
    "# we will reduce the model to just the generalizable characteritsitcs\n",
    "\n",
    "pred_train = modelFit.predict(x)\n",
    "pred_test = modelFit.predict(xt)\n",
    "\n",
    "#in sample accuracey with training data\n",
    "print(\"\\n\\nIn-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y, pred_train), 2))) #In-sample accuracy: 94.35%\n",
    "\n",
    "#out of sample accuracy with testing data\n",
    "print(\"\\n\\nOut-of-sample accuracy: %s%%\\n\\n\" \n",
    "% str(round(100*accuracy_score(yt, pred_test), 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#I think now i need to implement the actual test data data_pred\n",
    "\n",
    "#apparently some of the columns in the test data set have nan values so i need to fiind and remove them?\n",
    "data_pred.isnull().sum() #this shows me which variables have nans and how many they have\n",
    "\n",
    "\n",
    "#it looks like all of the values for Meal in data_pred are Nan?? maybe i just need to remove them or somethin\n",
    "data_pred.replace('nan', np.nan) #this replace the nan with the np.nan(NaN)\n",
    "\n",
    "#this remove the NaN  data_pred.dropna() , #dont do this i dont think its working lol\n",
    "\n",
    "#print(data_pred.dropna() )\n",
    "\n",
    "#took all of thevalues in the meal call and made it empty, so no Nan\n",
    "#need to make sure that these are empty because utimately this what we are trying to predict\n",
    "#did they get a meal (1)\n",
    "#did they not get a meal (0)\n",
    "#based on what else they ordered \n",
    "data_pred['meal'] = ''\n",
    "\n",
    "data_pred['meal'] = 0 #i just mad all the values in meal =0 because Idl it wasnt working with empty (said that the predictions had to atleast match the type of values)\n",
    "\n",
    "y2 = data_pred['meal'] #this meakes it so that the Depenedt variable (y) meal\n",
    "x2 = data_pred.drop(['meal', 'id', 'DateTime'], axis =1) #makes (x) the dependent variable everything but meal which is Y, also drops DateTime and Id because I dont think those are there to help predicy anything but just help find whatever\n",
    "\n",
    "#after checking again we can see that all of the Nans are gone\n",
    "data_pred.isnull().sum()\n",
    "\n",
    "pred_almost = modelFit.predict(x2)\n",
    "\n",
    "#now need to make sure that that values in pred are integers, when i chedked its saying that some of the numbers are being classfied as numpy.int64\n",
    "pred = [int(x) if isinstance(x, (str, np.int64, np.int32)) else x for x in pred_almost]\n",
    "#this helps to ensure that they are all integers \n",
    "\n",
    "print(\"\\n\\nTest (pred) data accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y2, pred), 2))) #the ys are then compared by the accuracy score against truth, and we see how often were right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "162720bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running test\n",
    "\n",
    "#for valid prediction test\n",
    "\n",
    "print(len(pred))\n",
    "\n",
    "#gonna make a function to check and see if the list has anything in it other than a float or an integer\n",
    "def checkNumbers(series):\n",
    "    for i in series:\n",
    "        if not isinstance(i,(float,int)):\n",
    "            print(\"Non-numeric element found:\", i, type(i))\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "checkNumbers(pred)\n",
    "#print(np.squeeze(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
